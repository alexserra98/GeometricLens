#!/bin/bash
#SBATCH --partition=DGX
#SBATCH --account=lade
#SBATCH --nodes=1
#SBATCH --time=6:00:00            
#SBATCH --ntasks-per-node=1       
#SBATCH --cpus-per-task=32           
#SBATCH --mem=80G                
#SBATCH --job-name=test
#SBATCH --gres=gpu:1 

source /etc/profile.d/modules.sh
module use /opt/nvidia/hpc_sdk/modulefiles/
module load nvhpc
source /u/area/ddoimo/anaconda3/bin/activate ./env_amd

export OMP_NUM_THREADS=32

NUM_GPUS=1

# Lora training
accelerate launch \
    --mixed_precision bf16 \
    --num_machines 1 \
    --num_processes $NUM_GPUS \
    cluster_analysis/extract_repr.py \
    --checkpoint_dir  "/u/area/ddoimo/ddoimo/llama/llama_v2/models_hf/llama-2-7b" \
    --use_slow_tokenizer \
    --preprocessing_num_workers 16 \
    --micro_batch_size 1 \
    --out_dir "./results" \
    --logging_steps 50 \
    --layer_interval 2 \
    --num_few_shots 0 \
    --save_distances --save_repr \
    --remove_duplicates \
    --use_last_token


