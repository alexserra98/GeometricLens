{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexserra98/miniconda3/envs/mcqa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from common.metadata_db import MetadataDB\n",
    "from common.tensor_storage import TensorStorage\n",
    "import numpy as p\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.features.features.Features"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "s = {\"A\",\"B\",\"C\",\"D\"}-{\"B\"}\n",
    "sample = random.choice(list(s))\n",
    "sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 7.39k/7.39k [00:00<00:00, 5.14MB/s]\n",
      "Downloading data: 100%|██████████| 1.25M/1.25M [00:00<00:00, 2.11MB/s]\n",
      "Downloading data: 100%|██████████| 160k/160k [00:00<00:00, 682kB/s]\n",
      "Downloading data: 100%|██████████| 151k/151k [00:00<00:00, 822kB/s]\n",
      "Generating train split: 100%|██████████| 9741/9741 [00:00<00:00, 284586.88 examples/s]\n",
      "Generating validation split: 100%|██████████| 1221/1221 [00:00<00:00, 117298.33 examples/s]\n",
      "Generating test split: 100%|██████████| 1140/1140 [00:00<00:00, 105171.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_dataset,Features,Value,Sequence,Array2D,Array3D,Array4D,Array5D\n",
    "context_feat = Features({'id':  Value(dtype='string', id=None),\n",
    " 'question':  Value(dtype='string', id=None),\n",
    " 'question_concept':  Value(dtype='string', id=None),\n",
    " 'choices': Sequence(feature={'label': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None)}, length=-1, id=None),\n",
    " 'answerKey': Value(dtype='string', id=None)})\n",
    "dataset = load_dataset('tau/commonsense_qa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 47.5M/47.5M [01:54<00:00, 416kB/s]\n",
      "Downloading data: 100%|██████████| 1.04M/1.04M [00:04<00:00, 228kB/s]\n",
      "Downloading data: 100%|██████████| 115k/115k [00:01<00:00, 105kB/s]\n",
      "Downloading data: 100%|██████████| 14.7k/14.7k [00:00<00:00, 16.1kB/s]\n",
      "Generating auxiliary_train split: 100%|██████████| 99842/99842 [00:00<00:00, 176848.13 examples/s]\n",
      "Generating test split: 100%|██████████| 1534/1534 [00:00<00:00, 197128.05 examples/s]\n",
      "Generating validation split: 100%|██████████| 170/170 [00:00<00:00, 40364.09 examples/s]\n",
      "Generating dev split: 100%|██████████| 5/5 [00:00<00:00, 1395.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('Stevross/mmlu','professional_law',trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '90b30172e645ff91f7171a048582eb8b',\n",
       " 'question': 'The townhouse was a hard sell for the realtor, it was right next to a high rise what?',\n",
       " 'question_concept': 'townhouse',\n",
       " 'choices': {'label': ['A', 'B', 'C', 'D', 'E'],\n",
       "  'text': ['suburban development',\n",
       "   'apartment building',\n",
       "   'bus stop',\n",
       "   'michigan',\n",
       "   'suburbs']},\n",
       " 'answerKey': ''}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RequestInstance():\n",
    "    prompt: str\n",
    "    letter_gold: str\n",
    "    token_gold: int = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"openbookqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C', 'D']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][0][\"choices\"][\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_question(row,shot=False):\n",
    "    prompt = f'Question: {row[\"question_stem\"]}\\n'\n",
    "    for letter, choice in zip(row[\"choices\"][\"label\"],row[\"choices\"][\"text\"]):\n",
    "        prompt += f'{letter}. {choice}\\n'\n",
    "    prompt += f'Answer: {row[\"choices\"][\"text\"][row[\"choices\"][\"label\"].index(row[\"answerKey\"])]}\\n\\n' if shot else  f'Answer:' \n",
    "    return prompt \n",
    "def construct_request_instance():\n",
    "    \"\"\"\n",
    "    Construct the request instances for the scenario\n",
    "    \"\"\"\n",
    "\n",
    "    output_mapping = dataset[\"test\"][0][\"choices\"][\"label\"]\n",
    "    \n",
    "    ri = []\n",
    "\n",
    "    dataset_test = dataset[\"test\"].select(range(5)) \\\n",
    "                                if 5 != -1 else dataset[\"test\"]\n",
    "    for row in tqdm(dataset_test, desc=\"Constructing Prompts\"):\n",
    "        prompt = f'The following are multiple choice questions (with answers) about {\"commonsense\"}.\\n\\n'\n",
    "        for i in range(5):\n",
    "            random_row = dataset[\"validation\"][i]\n",
    "            prompt += construct_question(random_row,shot=True)\n",
    "        prompt += construct_question(row)\n",
    "        ri.append(RequestInstance(prompt, row[\"answerKey\"]))\n",
    "    return ri, output_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constructing Prompts: 100%|██████████| 5/5 [00:00<00:00, 1070.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are multiple choice questions (with answers) about commonsense.\n",
      "\n",
      "Question: Frilled sharks and angler fish live far beneath the surface of the ocean, which is why they are known as\n",
      "A. Deep sea animals\n",
      "B. fish\n",
      "C. Long Sea Fish\n",
      "D. Far Sea Animals\n",
      "Answer: Deep sea animals\n",
      "\n",
      "Question: Gas can fill any container it is given, and liquid\n",
      "A. is standard weight and size\n",
      "B. is the opposite of variable\n",
      "C. only needs a few\n",
      "D. uses what it needs\n",
      "Answer: uses what it needs\n",
      "\n",
      "Question: When birds migrate south for the winter, they do it because\n",
      "A. they are genetically called to\n",
      "B. their children ask for them to\n",
      "C. it is important to their happiness\n",
      "D. they decide to each year\n",
      "Answer: they are genetically called to\n",
      "\n",
      "Question: If a person walks in the opposite direction of a compass arrow they are walking\n",
      "A. west\n",
      "B. north\n",
      "C. east\n",
      "D. south\n",
      "Answer: south\n",
      "\n",
      "Question: An example of lots kinetic energy would be\n",
      "A. Drinking a cold glass of water\n",
      "B. A snail moving across the sidewalk\n",
      "C. sitting without moving anywhere\n",
      "D. An aircraft taking a trip\n",
      "Answer: An aircraft taking a trip\n",
      "\n",
      "Question: A person wants to start saving money so that they can afford a nice vacation at the end of the year. After looking over their budget and expenses, they decide the best way to save money is to\n",
      "A. make more phone calls\n",
      "B. quit eating lunch out\n",
      "C. buy less with monopoly money\n",
      "D. have lunch with friends\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(construct_request_instance()[0][0].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'question': Value(dtype='string', id=None),\n",
       " 'question_concept': Value(dtype='string', id=None),\n",
       " 'choices': Sequence(feature={'label': Value(dtype='string', id=None), 'text': Value(dtype='int32', id=None)}, length=-1, id=None),\n",
       " 'answerKey': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Features.from_dict(\n",
    "# {'id': '075e483d21c29a511267ef62bedc0461',\n",
    "#  'question': 'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?',\n",
    "#  'question_concept': 'punishing',\n",
    "#  'choices': {'label': ['A', 'B', 'C', 'D', 'E'],\n",
    "#   'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']},\n",
    "#  'answerKey': 'A'}\n",
    "# )\n",
    "Features({'id':  Value(dtype='string', id=None),\n",
    " 'question':  Value(dtype='string', id=None),\n",
    " 'question_concept':  Value(dtype='string', id=None),\n",
    " 'choices': Sequence(feature={'label': Value(dtype='string', id=None), 'text': Value(dtype='int32', id=None)}, length=-1, id=None),\n",
    " 'answerKey': Value(dtype='string', id=None)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crfm-helm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
